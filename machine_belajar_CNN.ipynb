{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-belajar_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1xnifAyviPn",
        "colab_type": "text"
      },
      "source": [
        "Selections of code borrowed from: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3E-BPt55ijw",
        "colab_type": "text"
      },
      "source": [
        "1. To this runtime, upload the poli_data_format.csv file and the pretrained Indonesian word2vec model, id.bin: https://drive.google.com/file/d/0B0ZXk88koS2KQWxEemNNUHhnTWc/view (credit https://github.com/Kyubyong/wordvectors)\n",
        "2. Clone cleaned Indonesian tweets and stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Cdsk0hpLSz",
        "colab_type": "code",
        "outputId": "60d49e05-d6ec-4aec-ab35-e16a0a4fb8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "!git clone https://github.com/ridife/dataset-idsa.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'dataset-idsa' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzEoMn9Z1Vs7",
        "colab_type": "code",
        "outputId": "02e37a5d-ab90-4478-d082-2b3db8e0d935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/stopwords-iso/stopwords-id/master/stopwords-id.txt\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 01:40:57--  https://raw.githubusercontent.com/stopwords-iso/stopwords-id/master/stopwords-id.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6444 (6.3K) [text/plain]\n",
            "Saving to: ‘stopwords-id.txt.1’\n",
            "\n",
            "\rstopwords-id.txt.1    0%[                    ]       0  --.-KB/s               \rstopwords-id.txt.1  100%[===================>]   6.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-06 01:40:57 (82.8 MB/s) - ‘stopwords-id.txt.1’ saved [6444/6444]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaORFv_Oy7Ei",
        "colab_type": "text"
      },
      "source": [
        "3. Import necessary packages and download NLTK data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ZHQO5g4ICq",
        "colab_type": "code",
        "outputId": "0ea05118-e2b8-4749-d249-487c1546571e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfQqUopwa3y",
        "colab_type": "text"
      },
      "source": [
        "4. Import the data and fix up a bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0kmgdu_vhAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/dataset-idsa/Indonesian Sentiment Twitter Dataset Labeled.csv', sep='\\t', header=0)\n",
        "df2 = pd.read_csv('poli_data_format.csv', sep='\\t', header=0)\n",
        "df = pd.concat([df, df2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEiLyaQpN3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.columns = ['Label', 'Tweet']\n",
        "df = df[df.Label != 0] # since binary classification, leave out 'neutral' tweets\n",
        "df = df.reset_index(drop=True)\n",
        "df['Label'] = [1 if i==1 else 0 for i in df.Label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdxj6bE0E0O",
        "colab_type": "text"
      },
      "source": [
        "5. Preprocess a bit more, removing punctuation and stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LrYeqFcwfNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(text):\n",
        "  prepped_text = ''\n",
        "  prepped_text = re.sub('['+string.punctuation+']', '', text)\n",
        "  return prepped_text.lower()\n",
        "\n",
        "df['Tweet'] = df['Tweet'].apply(lambda x: prep(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQkE3fgS0I73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize tweets; English tokenizer, but Indonesian has similar enough tokenization rules\n",
        "tokens = [nltk.word_tokenize(sentence) for sentence in df.Tweet]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5J-y7Xf1nOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load up Indonesian stopwords\n",
        "stoplist = []\n",
        "with open('/content/stopwords-id.txt', 'r', encoding='utf-8') as inf:\n",
        "  for line in inf.readlines():\n",
        "    line = line[:-1]\n",
        "    stoplist.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXH8qrGH1-N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(tokens, stoplist):\n",
        "  return [token for token in tokens if token not in stoplist]\n",
        "\n",
        "filtered_tokens = [remove_stopwords(sentence, stoplist) for sentence in tokens]\n",
        "\n",
        "df['Tweet'] = [' '.join(sentence) for sentence in filtered_tokens]\n",
        "df['Tokens'] = filtered_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ju5peJl2477",
        "colab_type": "text"
      },
      "source": [
        "6. Set up one-hot encoded columns in dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ft95n523fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up one-hot encoding of labels\n",
        "pos = []\n",
        "neg = []\n",
        "for l in df.Label:\n",
        "    if l == 0:\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "    elif l == 1:\n",
        "        pos.append(1)\n",
        "        neg.append(0)\n",
        "\n",
        "df['Pos']= pos\n",
        "df['Neg']= neg\n",
        "df = df[['Tweet', 'Tokens', 'Label', 'Pos', 'Neg']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0g_Lg324SOh",
        "colab_type": "text"
      },
      "source": [
        "7. Split for train and test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO_3qmLf3PBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSeL5-eZqo1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "fed921c1-e1b2-48dc-ebbb-7f2ffb2556e2"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1670</th>\n",
              "      <td>tersilap fikir</td>\n",
              "      <td>[tersilap, fikir]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5335</th>\n",
              "      <td>suka dgr lagu mcm zaman saloma lagu irama mela...</td>\n",
              "      <td>[suka, dgr, lagu, mcm, zaman, saloma, lagu, ir...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>peliharalah dirimu siksaan yang khusus menimpa...</td>\n",
              "      <td>[peliharalah, dirimu, siksaan, yang, khusus, m...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3622</th>\n",
              "      <td>jejak digital menyakitkan menusuk2</td>\n",
              "      <td>[jejak, digital, menyakitkan, menusuk2]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>berlatih sempurna manusia yang sempurna susah ...</td>\n",
              "      <td>[berlatih, sempurna, manusia, yang, sempurna, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3772</th>\n",
              "      <td>gak kasian yang laju berangkat</td>\n",
              "      <td>[gak, kasian, yang, laju, berangkat]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>hahahaahhahahahahahahahaa la</td>\n",
              "      <td>[hahahaahhahahahahahahahaa, la]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>peduli apapun tanggapan orang tentangku terser...</td>\n",
              "      <td>[peduli, apapun, tanggapan, orang, tentangku, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>kemanusiaan derajatx hati simpati kemanusiaan ...</td>\n",
              "      <td>[kemanusiaan, derajatx, hati, simpati, kemanus...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>bergerak angin yg bertiup d kampung hitem warn...</td>\n",
              "      <td>[bergerak, angin, yg, bertiup, d, kampung, hit...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5134 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet  ... Neg\n",
              "1670                                     tersilap fikir  ...   1\n",
              "5335  suka dgr lagu mcm zaman saloma lagu irama mela...  ...   0\n",
              "5008  peliharalah dirimu siksaan yang khusus menimpa...  ...   0\n",
              "3622                 jejak digital menyakitkan menusuk2  ...   1\n",
              "3328  berlatih sempurna manusia yang sempurna susah ...  ...   1\n",
              "...                                                 ...  ...  ..\n",
              "3772                     gak kasian yang laju berangkat  ...   1\n",
              "5191                       hahahaahhahahahahahahahaa la  ...   0\n",
              "5226  peduli apapun tanggapan orang tentangku terser...  ...   0\n",
              "5390  kemanusiaan derajatx hati simpati kemanusiaan ...  ...   0\n",
              "860   bergerak angin yg bertiup d kampung hitem warn...  ...   1\n",
              "\n",
              "[5134 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lYOyyhEaQH",
        "colab_type": "text"
      },
      "source": [
        "7.1 Determine maximum train/test sentence length and number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNEsmTxK4acs",
        "colab_type": "code",
        "outputId": "e0a0f6a5-8f1d-4dc6-9a83-5eb8b6c9a915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "all_training_words = [word for tokens in df_train['Tokens'] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in df_train['Tokens']]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print('{} words total, with a vocabulary size of {}'.format(len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print('Max sentence length is {}'.format(max(training_sentence_lengths)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44662 words total, with a vocabulary size of 12805\n",
            "Max sentence length is 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8qkzxPm5LPa",
        "colab_type": "code",
        "outputId": "a12db8e7-088b-4418-a97f-eec4b596525a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "all_test_words = [word for tokens in df_test['Tokens'] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in df_test['Tokens']]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print('{} words total, with a vocabulary size of {}'.format(len(all_test_words), len(TEST_VOCAB)))\n",
        "print('Max sentence length is {}'.format(max(test_sentence_lengths)))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5117 words total, with a vocabulary size of 2813\n",
            "Max sentence length is 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Gb9Tlv51Pb",
        "colab_type": "text"
      },
      "source": [
        "8. Load Word2Vec and associated values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHXIbvS5zpi",
        "colab_type": "code",
        "outputId": "a4567747-499c-49f3-b9d9-eb8f16149140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "word2vec_path = '/content/id.bin'\n",
        "word2vec = models.Word2Vec.load(word2vec_path)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMfcHtGQFilB",
        "colab_type": "text"
      },
      "source": [
        "9. Get the Word2Vec embeddings; if a word cannot be found, generate a random vector for that word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lf-LVKL78zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['Tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRObfYi871Wm",
        "colab_type": "code",
        "outputId": "a216de5b-7d56-4109-a95e-f61d5866e279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, df_train, generate_missing=True)\n",
        "MAX_SEQUENCE_LENGTH = 28\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rsM13eHE6JA",
        "colab_type": "text"
      },
      "source": [
        "10. Tokenize and pad the word sequences for both train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvd5XH962YB",
        "colab_type": "code",
        "outputId": "bcd1bb1e-3f68-4f3d-930e-8c5c2b6f9692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(df['Tweet'].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(df_train['Tweet'].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.'.format(len(train_word_index)))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13688 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUh-gj3M7yQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyDJUxTJ8YQg",
        "colab_type": "code",
        "outputId": "8677c353-c92e-476f-f13a-19130366f4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIYZoBbn8bKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(df_test['Tweet'].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THjAFRow8hIo",
        "colab_type": "text"
      },
      "source": [
        "11. Set up the actual CNN model and other values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z26tliQi8iRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6] # five different filter sizes applied to each tweet\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7Gjc6f8k71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_names = ['Pos', 'Neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQoMNrpN8ovq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_train = df_train[label_names].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prAomFSj8uL-",
        "colab_type": "code",
        "outputId": "86e6b016-f6b2-4d98-e058-02daf7bd7979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        }
      },
      "source": [
        "model = ConvNet(train_embedding_weights,\n",
        "                MAX_SEQUENCE_LENGTH,\n",
        "                len(train_word_index)+1,\n",
        "                EMBEDDING_DIM, \n",
        "                len(list(label_names))\n",
        "                )"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 28)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 28, 300)      4106700     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 27, 200)      120200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 26, 200)      180200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 25, 200)      240200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 24, 200)      300200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 23, 200)      360200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_15 (Global (None, 200)          0           conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1000)         0           global_max_pooling1d_11[0][0]    \n",
            "                                                                 global_max_pooling1d_12[0][0]    \n",
            "                                                                 global_max_pooling1d_13[0][0]    \n",
            "                                                                 global_max_pooling1d_14[0][0]    \n",
            "                                                                 global_max_pooling1d_15[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1000)         0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          128128      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 2)            258         dropout_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,436,086\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 4,106,700\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgDfj39t89tY",
        "colab_type": "text"
      },
      "source": [
        "10. Train the CNN model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLLDgfXX88tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35FS7lz9BSI",
        "colab_type": "code",
        "outputId": "eafad0cc-3ffb-4466-ee64-e1488ee17aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "hist = model.fit(x_train,\n",
        "                 y_train,\n",
        "                 epochs=num_epochs,\n",
        "                 validation_split=0.1,\n",
        "                 shuffle=True,\n",
        "                 batch_size=batch_size,\n",
        "                 )"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4620 samples, validate on 514 samples\n",
            "Epoch 1/3\n",
            "4620/4620 [==============================] - 13s 3ms/step - loss: 0.7216 - acc: 0.6095 - val_loss: 0.6257 - val_acc: 0.6508\n",
            "Epoch 2/3\n",
            "4620/4620 [==============================] - 13s 3ms/step - loss: 0.5266 - acc: 0.7363 - val_loss: 0.6386 - val_acc: 0.6712\n",
            "Epoch 3/3\n",
            "4620/4620 [==============================] - 12s 3ms/step - loss: 0.4204 - acc: 0.8011 - val_loss: 0.6258 - val_acc: 0.7130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfErN89i_tRa",
        "colab_type": "text"
      },
      "source": [
        "11. Test the CNN model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZfWLBli_jPM",
        "colab_type": "code",
        "outputId": "270c817c-8648-495a-bee2-81a8bafcd8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r571/571 [==============================] - 1s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x177iO2_1UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [1, 0]\n",
        "\n",
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nB_As2yD7Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEgzJtYvDYOO",
        "colab_type": "code",
        "outputId": "46f6b95d-663c-4aef-be7d-a296415a7df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "df_test['Prediction'] = prediction_labels"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-sig6gbAzE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_classes = df_test.Prediction\n",
        "y_test = df_test.Label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45TTF9LOC3lN",
        "colab_type": "text"
      },
      "source": [
        "12. Make the confusion matrix and evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TPyg86FAzHl",
        "colab_type": "code",
        "outputId": "84658b49-4731-45de-a99a-8a07c7fc22da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "conf_matrix = pd.DataFrame(confusion_matrix(y_test, predicted_classes))\n",
        "print('Confusion Matrix')\n",
        "display(conf_matrix)\n",
        "\n",
        "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
        "\n",
        "print('\\n \\n Scores')\n",
        "scores = pd.DataFrame(data=[test_scores])\n",
        "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
        "scores = scores.T\n",
        "scores.columns = ['scores']\n",
        "display(scores)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>203</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1\n",
              "0  203  100\n",
              "1   76  192"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " Scores\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.691769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.657534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.716418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1</th>\n",
              "      <td>0.685714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             scores\n",
              "accuracy   0.691769\n",
              "precision  0.657534\n",
              "recall     0.716418\n",
              "f1         0.685714"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwWOQm4n_81L",
        "colab_type": "code",
        "outputId": "943dd96b-5931-455a-ad25-92076e79820a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "df_test.Label.value_counts()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    303\n",
              "1    268\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    }
  ]
}